R = runif(samplesize)
phat_group1 = 1 - sum(R>=p1 & R<1)/samplesize
phat_group1
dim(dataset)[1]
log(0.8)
set.seed(4123)
# nsim: number of simulations
nsim = 100000
# sample size per arm n0=n1
n0 = 500
# type i and ii errors
alpha=0.025; beta=0.2
z.alpha <- qnorm(1-alpha,0,1)
z.beta <-  qnorm(1-beta,0,1)
################################################################
# ODDS RATIO
f_OR <- function(samplesize,p0,p1){
# estimation of the probability of observing the composite event in control group
R = runif(samplesize)
phat_group0 = 1 - sum(R>=p0 & R<1)/samplesize
# estimation of the probability of observing the composite event in test group
R = runif(samplesize)
phat_group1 = 1 - sum(R>=p1 & R<1)/samplesize
# test odds ratio
TestOR_unpooled = log((phat_group1/(1-phat_group1))/(phat_group0/(1-phat_group0)))*((1/(phat_group0*(1-phat_group0))+ 1/(phat_group1*(1-phat_group1)))/samplesize)^(-1/2)
return(TestOR_unpooled)
}
################################################################
data$Test_Reject_CE = 0
data$Test_Reject_RE = 0
for(i in 1:dim(dataset)[1]){
data$Test_Reject_CE[i] <- sum(replicate(nsim,f_CE(n0,dataset$p0_ce[i],dataset$p1_ce[i]))< - z.alpha)/nsim
data$Test_Reject_RE[i] <- sum(replicate(nsim,f_CE(n0,dataset$p0_e1[i],dataset$p1_e1[i]))< - z.alpha)/nsim
}
set.seed(4123)
# nsim: number of simulations
nsim = 100000
# sample size per arm n0=n1
n0 = 500
# type i and ii errors
alpha=0.025; beta=0.2
z.alpha <- qnorm(1-alpha,0,1)
z.beta <-  qnorm(1-beta,0,1)
################################################################
# ODDS RATIO
f_OR <- function(samplesize,p0,p1){
# estimation of the probability of observing the composite event in control group
R = runif(samplesize)
phat_group0 = 1 - sum(R>=p0 & R<1)/samplesize
# estimation of the probability of observing the composite event in test group
R = runif(samplesize)
phat_group1 = 1 - sum(R>=p1 & R<1)/samplesize
# test odds ratio
TestOR_unpooled = log((phat_group1/(1-phat_group1))/(phat_group0/(1-phat_group0)))*((1/(phat_group0*(1-phat_group0))+ 1/(phat_group1*(1-phat_group1)))/samplesize)^(-1/2)
return(TestOR_unpooled)
}
################################################################
data$Test_Reject_CE = 0
data$Test_Reject_RE = 0
for(i in 1:dim(dataset)[1]){
data$Test_Reject_CE[i] <- sum(replicate(nsim,f_OR(n0,dataset$p0_ce[i],dataset$p1_ce[i]))< - z.alpha)/nsim
data$Test_Reject_RE[i] <- sum(replicate(nsim,f_OR(n0,dataset$p0_e1[i],dataset$p1_e1[i]))< - z.alpha)/nsim
}
i
i=1
dataset$p0_ce[i]
dataset$p1_ce[i]
sum(replicate(nsim,f_OR(n0,dataset$p0_ce[i],dataset$p1_ce[i]))< - z.alpha)/nsim
dataset$p0_e1[i]
dataset$p1_e1[i]
sum(replicate(nsim,f_OR(n0,dataset$p0_e1[i],dataset$p1_e1[i]))< - z.alpha)/nsim
dim(dataset)[1]
data$Test_Reject_CE = 0
data$Test_Reject_RE = 0
data$Test_Reject_CE = 0
data$Test_Reject_RE = 0
for(i in 1:dim(dataset)[1]){
data$Test_Reject_CE[i] <- sum(replicate(nsim,f_OR(n0,dataset$p0_ce[i],dataset$p1_ce[i]))< - z.alpha)/nsim
data$Test_Reject_RE[i] <- sum(replicate(nsim,f_OR(n0,dataset$p0_e1[i],dataset$p1_e1[i]))< - z.alpha)/nsim
print(i)
}
View(dataset)
data$Test_Reject_CE = 0
data$Test_Reject_RE = 0
dataset$Test_Reject_CE = 0
dataset$Test_Reject_RE = 0
dataset$Test_Reject_CE = 0
dataset$Test_Reject_RE = 0
View(dataset)
dataset$Test_Reject_CE = 0
dataset$Test_Reject_RE = 0
for(i in 1:dim(dataset)[1]){
dataset$Test_Reject_CE[i] <- sum(replicate(nsim,f_OR(n0,dataset$p0_ce[i],dataset$p1_ce[i]))< - z.alpha)/nsim
dataset$Test_Reject_RE[i] <- sum(replicate(nsim,f_OR(n0,dataset$p0_e1[i],dataset$p1_e1[i]))< - z.alpha)/nsim
print(i)
}
View(dataset)
dataset$Test_Reject_CE = 0
dataset$Test_Reject_RE = 0
for(i in 1:dim(dataset)[1]){
dataset$Test_Reject_CE[i] <- sum(replicate(nsim,f_OR(n0,dataset$p0_ce[i],dataset$p1_ce[i]))< - z.alpha)/nsim
dataset$Test_Reject_RE[i] <- sum(replicate(nsim,f_OR(n0,dataset$p0_e1[i],dataset$p1_e1[i]))< - z.alpha)/nsim
print(i)
}
sum(replicate(nsim,f_OR(n0,dataset$p0_ce[i],dataset$p1_ce[i]))< - z.alpha)/nsim
t0=Sys.time()
dataset$Test_Reject_CE = 0
dataset$Test_Reject_RE = 0
for(i in 1:dim(dataset)[1]){
dataset$Test_Reject_CE[i] <- sum(replicate(nsim,f_OR(n0,dataset$p0_ce[i],dataset$p1_ce[i]))< - z.alpha)/nsim
dataset$Test_Reject_RE[i] <- sum(replicate(nsim,f_OR(n0,dataset$p0_e1[i],dataset$p1_e1[i]))< - z.alpha)/nsim
print(i)
}
t1=Sys.time()-t0
##################################################################################
# Research project Vienna - Bcn
# Endpoint selection and sample size reassessment for composite binary endpoints
# Simulation study
##################################################################################
#########################################
# Preamble
#########################################
library(tidyverse)
library(tidyr)
library(plyr)
library(devtools)
install_github("CompARE-Composite/CompARE-package")
library(CompARE)
#########################################
# Define the set of scenarios
#########################################
# Scenarios
p0_e1 = c(0.1, 0.2)
p0_e2 = c(0.1, 0.2, 0.3)
OR1 = c(0.6, 0.7, 0.8, 0.9)
OR2 = c(0.65, 0.7, 0.8)
scenarios = expand_grid(p0_e1,p0_e2,OR1,OR2)
# Probabilities treat group
scenarios$p1_e1 = (scenarios$OR1*scenarios$p0_e1/(1-scenarios$p0_e1))/(1+(scenarios$OR1*scenarios$p0_e1/(1-scenarios$p0_e1)))
scenarios$p1_e2 = (scenarios$OR2*scenarios$p0_e2/(1-scenarios$p0_e2))/(1+(scenarios$OR2*scenarios$p0_e2/(1-scenarios$p0_e2)))
# Calculate the correlation bounds
scenarios$min_corr0 = mapply(lower_corr,scenarios$p0_e1,scenarios$p0_e2)
scenarios$min_corr1 = mapply(lower_corr,scenarios$p1_e1,scenarios$p1_e2)
scenarios$max_corr0 = mapply(upper_corr,scenarios$p0_e1,scenarios$p0_e2)
scenarios$max_corr1 = mapply(upper_corr,scenarios$p1_e1,scenarios$p1_e2)
scenarios$min_corr = pmax(scenarios$min_corr0,scenarios$min_corr1)
scenarios$max_corr = pmin(scenarios$max_corr0,scenarios$max_corr1)
scenarios1 = scenarios; scenarios2 = scenarios; scenarios3 = scenarios
scenarios3$corr = (scenarios3$max_corr)/3
scenarios2$corr = (scenarios2$max_corr)/2
scenarios1$corr = scenarios1$max_corr
scenarios$corr = 0
dataset = rbind(scenarios,scenarios1,scenarios2,scenarios3)
# Calculate CE Probabilities
dataset$p0_ce = mapply(prob_cbe, p_e1=dataset$p0_e1, p_e2=dataset$p0_e2, rho=dataset$corr)
dataset$p1_ce = mapply(prob_cbe, p_e1=dataset$p1_e1, p_e2=dataset$p1_e2, rho=dataset$corr)
# Calculate ARE
dataset$ARE = mapply(ARE_cbe, p0_e1=dataset$p0_e1, p0_e2=dataset$p0_e2, eff_e1=dataset$OR1, eff_e2=dataset$OR2, rho=dataset$corr)
dataset$decision = ifelse(dataset$ARE<1, "RE", "CE")
#########################################
# Simulations
#########################################
set.seed(4123)
# nsim: number of simulations
nsim = 100000
# sample size per arm n0=n1
n0 = 500
# type i and ii errors
alpha=0.025; beta=0.2
z.alpha <- qnorm(1-alpha,0,1)
z.beta <-  qnorm(1-beta,0,1)
################################################################
# ODDS RATIO
f_OR <- function(samplesize,p0,p1){
# estimation of the probability of observing the composite event in control group
R = runif(samplesize)
phat_group0 = 1 - sum(R>=p0 & R<1)/samplesize
# estimation of the probability of observing the composite event in test group
R = runif(samplesize)
phat_group1 = 1 - sum(R>=p1 & R<1)/samplesize
# test odds ratio
TestOR_unpooled = log((phat_group1/(1-phat_group1))/(phat_group0/(1-phat_group0)))*((1/(phat_group0*(1-phat_group0))+ 1/(phat_group1*(1-phat_group1)))/samplesize)^(-1/2)
return(TestOR_unpooled)
}
################################################################
t0=Sys.time()
dataset$Test_Reject_CE = 0
dataset$Test_Reject_RE = 0
for(i in 1:dim(dataset)[1]){
dataset$Test_Reject_CE[i] <- sum(replicate(nsim,f_OR(n0,dataset$p0_ce[i],dataset$p1_ce[i]))< - z.alpha)/nsim
dataset$Test_Reject_RE[i] <- sum(replicate(nsim,f_OR(n0,dataset$p0_e1[i],dataset$p1_e1[i]))< - z.alpha)/nsim
print(i)
}
t1=Sys.time()-t0
t1
setwd("C:/Users/Marta/Dropbox/C5/Scripts/GitKraken/CBE_selection")
rm(alpha,beta,i,nsim,z.alpha,z.beta,OR1,OR2,p0_ce,p0_e1,p0_e2,f_OR)
save.image("C:/Users/Marta/Dropbox/C5/Scripts/GitKraken/CBE_selection/results1.RData")
View(dataset)
dataset$diff_powers = dataset$Test_Reject_CE - dataset$Test_Reject_RE
dataset$gain_power = ifelse(diff_powers>0, "CE", "RE")
dataset$diff_powers = dataset$Test_Reject_CE - dataset$Test_Reject_RE
dataset$gain_power = ifelse(dataset$diff_powers>0, "CE", "RE")
summary(dataset)
(dataset$gain_power == dataset$power)
(dataset$gain_power == dataset$decision)
dataset$comparison = (dataset$gain_power == dataset$decision)
dataset$comparison
dataset[,dataset$comparison++F]
dataset[,dataset$comparison==F]
subset(dataset,dataset$comparison==FALSE)
summary(subset(dataset,dataset$comparison==FALSE))
View(dataset)
save.image("C:/Users/Marta/Dropbox/C5/Scripts/GitKraken/CBE_selection/results.RData")
rm(alpha,beta,i,nsim,z.alpha,z.beta,OR1,OR2,p0_ce,p0_e1,p0_e2,f_OR,scenarios,scenarios1,scenarios2,scenarios3)
save.image("C:/Users/Marta/Dropbox/C5/Scripts/GitKraken/CBE_selection/results.RData")
source('C:/Users/Marta/Dropbox/C5/Scripts/GitKraken/CBE_selection/sim_functions.R')
rmultinom(1,500,(0.2,0.3,0.5))
rmultinom(1,500,(0.25,0.25,0.55,0.25))
rmultinom(1,500,c(0.25,0.25,0.55,0.25))
rmultinom(2,500,c(0.25,0.25,0.55,0.25))
rmultinom(2,500,c(0.25,0.25))
rmultinom(2,500,c(0.25,0.25))
rmultinom(2,500,c(0.25,0.25))
rmultinom(2,500,c(0.25,0.25))
rmultinom(2,500,c(0.25,0.25))
##############################################################
i=1
p0_e1[i] = dataset$p0_e1[i]
##################################################################################
# Research project Vienna - Bcn
# Endpoint selection and sample size reassessment for composite binary endpoints
# Simulation study
##################################################################################
setwd("C:/Users/Marta/Dropbox/C5/Scripts/GitKraken/CBE_selection")
source('C:/Users/Marta/Dropbox/C5/Scripts/GitKraken/CBE_selection/sim_functions.R')
#########################################
# Preamble
#########################################
library(tidyverse)
library(tidyr)
library(plyr)
library(devtools)
install_github("CompARE-Composite/CompARE-package")
library(CompARE)
#########################################
# Define the set of scenarios
#########################################
# Scenarios
p0_e1 = c(0.1, 0.2)
p0_e2 = c(0.1, 0.2, 0.3)
OR1 = c(0.6, 0.7, 0.8, 0.9)
OR2 = c(0.65, 0.7, 0.8)
scenarios = expand_grid(p0_e1,p0_e2,OR1,OR2)
# Probabilities treat group
scenarios$p1_e1 = (scenarios$OR1*scenarios$p0_e1/(1-scenarios$p0_e1))/(1+(scenarios$OR1*scenarios$p0_e1/(1-scenarios$p0_e1)))
scenarios$p1_e2 = (scenarios$OR2*scenarios$p0_e2/(1-scenarios$p0_e2))/(1+(scenarios$OR2*scenarios$p0_e2/(1-scenarios$p0_e2)))
# Calculate the correlation bounds
scenarios$min_corr0 = mapply(lower_corr,scenarios$p0_e1,scenarios$p0_e2)
scenarios$min_corr1 = mapply(lower_corr,scenarios$p1_e1,scenarios$p1_e2)
scenarios$max_corr0 = mapply(upper_corr,scenarios$p0_e1,scenarios$p0_e2)
scenarios$max_corr1 = mapply(upper_corr,scenarios$p1_e1,scenarios$p1_e2)
scenarios$min_corr = pmax(scenarios$min_corr0,scenarios$min_corr1)
scenarios$max_corr = pmin(scenarios$max_corr0,scenarios$max_corr1)
scenarios1 = scenarios; scenarios2 = scenarios; scenarios3 = scenarios
scenarios3$corr = (scenarios3$max_corr)/3
scenarios2$corr = (scenarios2$max_corr)/2
scenarios1$corr = scenarios1$max_corr
scenarios$corr = 0
dataset = rbind(scenarios,scenarios1,scenarios2,scenarios3)
# Calculate CE Probabilities
dataset$p0_ce = mapply(prob_cbe, p_e1=dataset$p0_e1, p_e2=dataset$p0_e2, rho=dataset$corr)
dataset$p1_ce = mapply(prob_cbe, p_e1=dataset$p1_e1, p_e2=dataset$p1_e2, rho=dataset$corr)
# Calculate ARE
dataset$ARE = mapply(ARE_cbe, p0_e1=dataset$p0_e1, p0_e2=dataset$p0_e2, eff_e1=dataset$OR1, eff_e2=dataset$OR2, rho=dataset$corr)
dataset$decision = ifelse(dataset$ARE<1, "RE", "CE")
##############################################################
i=1
p0_e1 = dataset$p0_e1[i]
p1_e1 = dataset$p1_e1[i]
p0_e2 = dataset$p0_e2[i]
p1_e2 = dataset$p1_e2[i]
p0_ce = dataset$p0_ce[i]
p1_ce = dataset$p1_ce[i]
samplesize=500
# CONTROL GROUP
# Endpoint 1
R = runif(samplesize)
X1_0 = sum(R<p0_e1 & R>0)
# Endpoint 2
R = runif(X1_0)
p_aux=(p0_e1+p0_e2-p0_ce)/p0_e1
X21_0 = sum(R<p_aux & R>0)
# phat_group0 = X21_0/samplesize
R = runif(samplesize-X1_0)
p_aux=(p0_ce-p0_e1)/(1-p0_e1)
X20_0 = sum(R<p_aux & R>0)
# phat_group0 = X20_0/samplesize
# INTERVENTION GROUP
# Endpoint 1
R = runif(samplesize)
X1_1 = sum(R<p1_e1 & R>0)
# phat_group0 = X1_1/samplesize
# Endpoint 2
R = runif(X1_1)
p_aux=(p1_e1+p1_e2-p1_ce)/p1_e1
X21_1 = sum(R<p_aux & R>0)
# phat_group0 = X21_1/samplesize
R = runif(samplesize-X1_1)
p_aux=(p1_ce-p1_e1)/(1-p1_e1)
X20_1 = sum(R<p_aux & R>0)
# phat_group0 = X20_1/samplesize
X1 = c(X1_0,X1_1)
X2 = c(X21_0,X20_0,X21_1,X20_1)
cor(X1,X2)
rep(0,2)
X1 = c(X1_0,rep(0,samplesize-X1_0),X1_1,rep(0,samplesize-X1_1))
X2 = c(X21_0,X20_0,X21_1,X20_1)
cor(X1,X2)
X1 = c(X1_0,rep(0,samplesize-X1_0),X1_1,rep(0,samplesize-X1_1))
X2 = c(X21_0,X20_0,rep(0,samplesize-X21_0-X20_0),X21_1,X20_1,rep(0,samplesize-X21_1-X20_1))
samplesize
cor(X1,X2)
View(dataset)
samplesize
X1_0
rep(0,samplesize-X1_0)
rep(1,X1_0)
X1 = c(rep(1,X1_0),rep(0,samplesize-X1_0),rep(1,X1_1),rep(0,samplesize-X1_1))
X1 = c(rep(1,X1_0),rep(0,samplesize-X1_0),rep(1,X1_1),rep(0,samplesize-X1_1))
X2 = c(rep(1,X21_0),rep(1,X20_0),rep(0,samplesize-X21_0-X20_0),rep(1,X21_1),rep(1,X20_1),rep(0,samplesize-X21_1-X20_1))
cor(X1,X2)
##############################################################
i=1
p0_e1 = dataset$p0_e1[i]
p1_e1 = dataset$p1_e1[i]
p0_e2 = dataset$p0_e2[i]
p1_e2 = dataset$p1_e2[i]
p0_ce = dataset$p0_ce[i]
p1_ce = dataset$p1_ce[i]
samplesize=500
dataset$corr[i]
# CONTROL GROUP
# Endpoint 1
R = runif(samplesize)
X1_0 = sum(R<p0_e1 & R>0)
# phat_group0 = X1_0/samplesize
# Endpoint 2
R = runif(X1_0)
p_aux=(p0_e1+p0_e2-p0_ce)/p0_e1
X21_0 = sum(R<p_aux & R>0)
# phat_group0 = X21_0/samplesize
R = runif(samplesize-X1_0)
p_aux=(p0_ce-p0_e1)/(1-p0_e1)
X20_0 = sum(R<p_aux & R>0)
# phat_group0 = X20_0/samplesize
# INTERVENTION GROUP
# Endpoint 1
R = runif(samplesize)
X1_1 = sum(R<p1_e1 & R>0)
# phat_group0 = X1_1/samplesize
# Endpoint 2
R = runif(X1_1)
p_aux=(p1_e1+p1_e2-p1_ce)/p1_e1
X21_1 = sum(R<p_aux & R>0)
# phat_group0 = X21_1/samplesize
R = runif(samplesize-X1_1)
p_aux=(p1_ce-p1_e1)/(1-p1_e1)
X20_1 = sum(R<p_aux & R>0)
# phat_group0 = X20_1/samplesize
X1 = c(rep(1,X1_0),rep(0,samplesize-X1_0),rep(1,X1_1),rep(0,samplesize-X1_1))
X2 = c(rep(1,X21_0),rep(1,X20_0),rep(0,samplesize-X21_0-X20_0),rep(1,X21_1),rep(1,X20_1),rep(0,samplesize-X21_1-X20_1))
cor(X1,X2)
X1 = c(rep(1,X1_0),rep(0,samplesize-X1_0))
X2 = c(rep(1,X21_0),rep(1,X20_0),rep(0,samplesize-X21_0-X20_0))
cor(X1,X2)
cor(X1,X2)
X1
X2
p1_e1
p1_e2
phat_group0 = X21_1/samplesize
# INTERVENTION GROUP
# Endpoint 1
R = runif(samplesize)
X1_1 = sum(R<p1_e1 & R>0)
phat_group0 = X1_1/samplesize
phat_group0
sum(X1)/samplesize
sum(X2)/samplesize
# phat_group0 = X1_0
sum(R<p0_e1 & R>0)/samplesize
p0_e1
R = runif(X1_0)
p_aux=(p0_e1+p0_e2-p0_ce)/p0_e1
X21_0 = sum(R<p_aux & R>0)
X21_0/samplesize
p_aux
# Endpoint 2
R = runif(X1_0)
p_aux=(p0_e1+p0_e2-p0_ce)/p0_e1
X21_0 = sum(R<p_aux & R>0)
sum(R<p_aux & R>0)
X21_0 = sum(R<p_aux & R>0)
X21_0/samplesize
i=1
p0_e1 = dataset$p0_e1[i]
p1_e1 = dataset$p1_e1[i]
p0_e2 = dataset$p0_e2[i]
p1_e2 = dataset$p1_e2[i]
p0_ce = dataset$p0_ce[i]
p1_ce = dataset$p1_ce[i]
samplesize=500
# CONTROL GROUP
# Endpoint 1
R = runif(samplesize)
X1_0 = sum(R<p0_e1 & R>0)
# phat_group0 = X1_0/samplesize
# Endpoint 2
R = runif(X1_0)
p_aux=(p0_e1+p0_e2-p0_ce)/p0_e1
X21_0 = sum(R<p_aux & R>0)
# phat_group0 = X21_0/samplesize
R = runif(samplesize-X1_0)
p_aux=(p0_ce-p0_e1)/(1-p0_e1)
X20_0 = sum(R<p_aux & R>0)
# phat_group0 = X20_0/samplesize
# INTERVENTION GROUP
# Endpoint 1
R = runif(samplesize)
X1_1 = sum(R<p1_e1 & R>0)
# phat_group0 = X1_1/samplesize
# Endpoint 2
R = runif(X1_1)
p_aux=(p1_e1+p1_e2-p1_ce)/p1_e1
X21_1 = sum(R<p_aux & R>0)
# phat_group0 = X21_1/samplesize
R = runif(samplesize-X1_1)
p_aux=(p1_ce-p1_e1)/(1-p1_e1)
X20_1 = sum(R<p_aux & R>0)
# phat_group0 = X20_1/samplesize
X1 = c(rep(1,X1_0),rep(0,samplesize-X1_0),rep(1,X1_1),rep(0,samplesize-X1_1))
X2 = c(rep(1,X21_0),rep(1,X20_0),rep(0,samplesize-X21_0-X20_0),rep(1,X21_1),rep(1,X20_1),rep(0,samplesize-X21_1-X20_1))
X1
X2
cor(X1,X2)
X1-X2
rmultinom(2,500,c(0.25,0.25))
rmultinom(2,500,c(0.75,0.25))
rmultinom(2,500,c(0.75,0))
rmultinom(2,500,c(0.75,0.25))
382/(382+118)
360/(360+140)
rmultinom(2,500,c(0.75,0.25,0.3,0.7))
rmultinom(2,500,c(0.75,0.25))
i=1
p0_e1 = dataset$p0_e1[i]
p1_e1 = dataset$p1_e1[i]
p0_e2 = dataset$p0_e2[i]
p1_e2 = dataset$p1_e2[i]
p0_ce = dataset$p0_ce[i]
p1_ce = dataset$p1_ce[i]
samplesize=500
s1_group0 = p0_e1+p0_e2-p0_ce
s2_group0 = p0_e1 - p0_e1+p0_e2-p0_ce
s3_group0 = p0_e2 - p0_e1+p0_e2-p0_ce
s4_group0 = 1 - s1_group0 - s2_group0 - s3_group0
rmultinom(2,500,c(s1_group0,s2_group0,s3_group0,s4_group0))
s1_group0
s2_group0
s1_group0 = p0_e1+p0_e2-p0_ce
s2_group0 = p0_e1 - (p0_e1+p0_e2-p0_ce)
s3_group0 = p0_e2 - (p0_e1+p0_e2-p0_ce)
s4_group0 = 1 - s1_group0 - s2_group0 - s3_group0
rmultinom(2,500,c(s1_group0,s2_group0,s3_group0,s4_group0))
rmultinom(4,500,c(s1_group0,s2_group0,s3_group0,s4_group0))
rmultinom(2,500,c(s1_group0,s2_group0,s3_group0,s4_group0))
rmultinom(1,500,c(s1_group0,s2_group0,s3_group0,s4_group0))
s1_group0
3/500
35/500
s2_group0
49/500
s3_group0
rmultinom(1,500,c(s1_group0,s2_group0,s3_group0,s4_group0))
rmultinom(1,500,c(s1_group0,s2_group0,s3_group0,s4_group0))
corr_est =( s1_group0*s4_group0-(s2_group0*s3_group0))/sqrt((s1_group0+s3_group0)*(s2_group0+s4_group0)*(s1_group0+s2_group0)*(s3_group0+s4_group0))
corr_est
rmultinom(1,500,c(s1_group0,s2_group0,s3_group0,s4_group0))
sm = rmultinom(1,500,c(s1_group0,s2_group0,s3_group0,s4_group0))
sm
sm[1]
sm[2]
sm = rmultinom(1,500,c(s1_group0,s2_group0,s3_group0,s4_group0))
corr_est =(sm[1]*sm[4]-(sm[2]*sm[3]))/sqrt((sm[1]+sm[3])*(sm[2]+sm[4])*(sm[1]+sm[2])*(sm[3]+sm[4]))
corr_est
i=1
p0_e1 = dataset$p0_e1[i]
p1_e1 = dataset$p1_e1[i]
p0_e2 = dataset$p0_e2[i]
p1_e2 = dataset$p1_e2[i]
p0_ce = dataset$p0_ce[i]
p1_ce = dataset$p1_ce[i]
samplesize=500
# dataset$corr[i]
s1_group0 = p0_e1+p0_e2-p0_ce
s2_group0 = p0_e1 - (p0_e1+p0_e2-p0_ce)
s3_group0 = p0_e2 - (p0_e1+p0_e2-p0_ce)
s4_group0 = 1 - s1_group0 - s2_group0 - s3_group0
sm = rmultinom(1,500,c(s1_group0,s2_group0,s3_group0,s4_group0))
corr_est =(sm[1]*sm[4]-(sm[2]*sm[3]))/sqrt((sm[1]+sm[3])*(sm[2]+sm[4])*(sm[1]+sm[2])*(sm[3]+sm[4]))
(corr_est)
