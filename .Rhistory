scenarios$p1_e2 = (scenarios$OR2*scenarios$p0_e2/(1-scenarios$p0_e2))/(1+(scenarios$OR2*scenarios$p0_e2/(1-scenarios$p0_e2)))
# Calculate the correlation bounds
scenarios$min_corr0 = mapply(lower_corr,scenarios$p0_e1,scenarios$p0_e2)
scenarios$min_corr1 = mapply(lower_corr,scenarios$p1_e1,scenarios$p1_e2)
scenarios$max_corr0 = mapply(upper_corr,scenarios$p0_e1,scenarios$p0_e2)
scenarios$max_corr1 = mapply(upper_corr,scenarios$p1_e1,scenarios$p1_e2)
scenarios$min_corr = pmax(scenarios$min_corr0,scenarios$min_corr1)
scenarios$max_corr = pmin(scenarios$max_corr0,scenarios$max_corr1)
corr = c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8)
dataset = expand_grid(scenarios, corr)
dataset = subset(dataset, dataset$corr < dataset$max_corr & dataset$corr > dataset$min_corr)
# Calculate CE Probabilities
dataset$p0_ce = mapply(prob_cbe, p_e1=dataset$p0_e1, p_e2=dataset$p0_e2, rho=dataset$corr)
dataset$p1_ce = mapply(prob_cbe, p_e1=dataset$p1_e1, p_e2=dataset$p1_e2, rho=dataset$corr)
dataset$OR_ce = (dataset$p1_ce/(1-dataset$p1_ce))/(dataset$p0_ce/(1-dataset$p0_ce))
# Calculate ARE
dataset$ARE = mapply(ARE_cbe, p0_e1=dataset$p0_e1, p0_e2=dataset$p0_e2, eff_e1=dataset$OR1, eff_e2=dataset$OR2, rho=dataset$corr)
dataset$decision = ifelse(dataset$ARE<1, "RE", "CE")
# Calculate Sample size (total sample size, n=2*n0=2*n1)
alpha=0.05; beta=0.2
dataset$samplesize_e1 = mapply(samplesize_OR,p0=dataset$p0_e1, OR=dataset$OR1, alpha=alpha, beta=beta)
# dataset$samplesize_e1 = samplesize_OR(p0=dataset$p0_e1, OR=dataset$OR1, alpha=alpha, beta=beta)
rm(OR1,OR2,p0_e1,p0_e2,corr,scenarios)
# Vector empirical powers and significance level
dataset$Test_Power_CE <- NA
dataset$Test_Power_RE <- NA
dataset$Test_Power_ES <- NA
dataset$Test_Reject_CE <- NA
dataset$Test_Reject_RE <- NA
dataset$Test_Reject_ES <- NA
set.seed(4123)
# nsim: number of simulations
nsim = 100000
# type i and ii errors
z.alpha <- qnorm(1-alpha,0,1)
z.beta <-  qnorm(1-beta,0,1)
dataset$Test_Power_ES2 <- NA
dataset$decision_ES2 <- NA
# for(i in 1:dim(dataset)[1]){
aux <- c()
for(i in 1:5){
aux <- tryCatch(rowSums(replicate(2,f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
)< c(-z.alpha,0))/nsim,
error=function(e){NA})
dataset$Test_Power_ES2[i]<-aux[1]
dataset$decision_ES[i]<-aux[2]
print(i)
}
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
dataset$Test_Power_ES2 <- NA
dataset$decision_ES2 <- NA
# for(i in 1:dim(dataset)[1]){
# aux <- c()
for(i in 1:5){
aux <- tryCatch(rowSums(replicate(2,f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
)< c(-z.alpha,0))/nsim,
error=function(e){NA})
dataset$Test_Power_ES2[i]<-aux[1]
dataset$decision_ES2[i]<-aux[2]
print(i)
}
aux
aux <- rowSums(replicate(2,f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i]))< c(-z.alpha,0))/nsim
aux
dataset$Test_Power_ES2 <- NA
dataset$decision_ES2 <- NA
# for(i in 1:dim(dataset)[1]){
# aux <- c()
for(i in 1:5){
aux <- rowSums(replicate(nsim,f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i]))< c(-z.alpha,0))/nsim
dataset$Test_Power_ES2[i]<- aux[1]
dataset$decision_ES2[i]<- aux[2]
print(i)
}
windows()
p <- list()
q <- list()
it = 1
for(i in 1:2){
sub=subset(dataset,dataset$scenario==i)
p[[it]] <-ggplot(sub, aes(x=corr, y=Test_Power_ES, color=as.factor(decision)))  +
geom_point(size=2) + labs(y = "Test_Power_ES", x="Correlation", color="Decision") + coord_cartesian(ylim = c(0.75, 1))
p[[it+1]] <-ggplot(sub, aes(x=corr, y=Test_Power_CE, color=as.factor(decision)))  +
geom_point(size=2) + labs(y = "Test_Power_CE", x="Correlation", color="Decision") + coord_cartesian(ylim = c(0.75, 1))
p[[it+2]] <-ggplot(sub, aes(x=corr, y=Test_Power_RE, color=as.factor(decision)))  +
geom_point(size=2) + labs(y = "Test_Power_RE", x="Correlation", color="Decision") + coord_cartesian(ylim = c(0.75, 1))
it=it+3
}
marrangeGrob(p,ncol=3,nrow=1)
windows()
p <- list()
q <- list()
it = 1
for(i in 1:2){
sub=subset(dataset,dataset$scenario==i)
p[[it]] <-ggplot(sub, aes(x=corr, y=Test_Power_ES2, color=as.factor(decision)))  +
geom_point(size=2) + labs(y = "Test_Power_ES", x="Correlation", color="Decision") + coord_cartesian(ylim = c(0.75, 1))
p[[it+1]] <-ggplot(sub, aes(x=corr, y=Test_Power_CE, color=as.factor(decision)))  +
geom_point(size=2) + labs(y = "Test_Power_CE", x="Correlation", color="Decision") + coord_cartesian(ylim = c(0.75, 1))
p[[it+2]] <-ggplot(sub, aes(x=corr, y=Test_Power_RE, color=as.factor(decision)))  +
geom_point(size=2) + labs(y = "Test_Power_RE", x="Correlation", color="Decision") + coord_cartesian(ylim = c(0.75, 1))
it=it+3
}
marrangeGrob(p,ncol=3,nrow=1)
aux
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i])
f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i]))< c(-z.alpha,1)
aux <- rowSums(replicate(nsim,f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i]))< c(-z.alpha,1))/nsim
aux
for(i in 1:25){
aux <- rowSums(replicate(nsim,f_ES2(samplesize=dataset$samplesize_e1[i]/2,
p0_e1=dataset$p0_e1[i],p1_e1=dataset$p1_e1[i],
OR1=dataset$OR1[i],
p0_e2=dataset$p0_e2[i],p1_e2=dataset$p1_e2[i],
OR2=dataset$OR2[i],
p0_ce=dataset$p0_ce[i],p1_ce=dataset$p1_ce[i],
upp=dataset$max_corr[i],low=dataset$min_corr[i]))< c(-z.alpha,1))/nsim
dataset$Test_Power_ES2[i]<- aux[1]
dataset$decision_ES2[i]<- 1-aux[2]
print(i)
}
##################################################################################
# Differences with respect to previous versions
##################################################################################
# - consider less scenarios for OR2, p1, p2
# - add scenarios correlation between components
# - computation sample size according to E1
#########################################
# Preamble
#########################################
library(tidyverse)
library(tidyr)
library(plyr)
library(devtools)
install_github("CompARE-Composite/CompARE-package")
library(CompARE)
#########################################
# Define the set of scenarios
#########################################
# Scenarios
p0_e1 = c(0.1, 0.2)
p0_e2 = c(0.1, 0.25)
OR1 = c(0.6, 0.7, 0.8)
OR2 = c(0.75, 0.8)
scenarios = expand_grid(p0_e1,p0_e2,OR1,OR2)
scenarios$scenario = 1:dim(scenarios)[1]
# Probabilities treat group
scenarios$p1_e1 = (scenarios$OR1*scenarios$p0_e1/(1-scenarios$p0_e1))/(1+(scenarios$OR1*scenarios$p0_e1/(1-scenarios$p0_e1)))
scenarios$p1_e2 = (scenarios$OR2*scenarios$p0_e2/(1-scenarios$p0_e2))/(1+(scenarios$OR2*scenarios$p0_e2/(1-scenarios$p0_e2)))
# Calculate the correlation bounds
scenarios$min_corr0 = mapply(lower_corr,scenarios$p0_e1,scenarios$p0_e2)
scenarios$min_corr1 = mapply(lower_corr,scenarios$p1_e1,scenarios$p1_e2)
scenarios$max_corr0 = mapply(upper_corr,scenarios$p0_e1,scenarios$p0_e2)
scenarios$max_corr1 = mapply(upper_corr,scenarios$p1_e1,scenarios$p1_e2)
scenarios$min_corr = pmax(scenarios$min_corr0,scenarios$min_corr1)
scenarios$max_corr = pmin(scenarios$max_corr0,scenarios$max_corr1)
corr = c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8)
dataset = expand_grid(scenarios, corr)
dataset = subset(dataset, dataset$corr < dataset$max_corr & dataset$corr > dataset$min_corr)
# Calculate CE Probabilities
dataset$p0_ce = mapply(prob_cbe, p_e1=dataset$p0_e1, p_e2=dataset$p0_e2, rho=dataset$corr)
dataset$p1_ce = mapply(prob_cbe, p_e1=dataset$p1_e1, p_e2=dataset$p1_e2, rho=dataset$corr)
dataset$OR_ce = (dataset$p1_ce/(1-dataset$p1_ce))/(dataset$p0_ce/(1-dataset$p0_ce))
# Calculate ARE
dataset$ARE = mapply(ARE_cbe, p0_e1=dataset$p0_e1, p0_e2=dataset$p0_e2, eff_e1=dataset$OR1, eff_e2=dataset$OR2, rho=dataset$corr)
dataset$decision = ifelse(dataset$ARE<1, "RE", "CE")
# Calculate Sample size (total sample size, n=2*n0=2*n1)
alpha=0.05; beta=0.2
dataset$samplesize_e1 = mapply(samplesize_OR,p0=dataset$p0_e1, OR=dataset$OR1, alpha=alpha, beta=beta)
# dataset$samplesize_e1 = samplesize_OR(p0=dataset$p0_e1, OR=dataset$OR1, alpha=alpha, beta=beta)
rm(OR1,OR2,p0_e1,p0_e2,corr,scenarios)
# Vector empirical powers and significance level
dataset$Test_Power_CE <- NA
dataset$Test_Power_RE <- NA
dataset$Test_Power_ES <- NA
dataset$decision_ES <- NA
dataset$Test_Reject_CE <- NA
dataset$Test_Reject_RE <- NA
dataset$Test_Reject_ES <- NA
rm(list = ls())
# setwd("C:/Users/mbofi/Dropbox/C5/Scripts/GitKraken/CBE_selection")
# source('C:/Users/mbofi/Dropbox/C5/Scripts/GitKraken/CBE_selection/Functions.R')
# source('C:/Users/mbofi/Dropbox/C5/Scripts/GitKraken/CBE_selection/sim_functions.R')
# source('C:/Users/mbofi/Dropbox/C5/Scripts/GitKraken/CBE_selection/sim_functions_ce.R')
setwd("C:/Users/Marta/Dropbox/C5/Scripts/GitKraken/CBE_selection")
source('C:/Users/Marta/Dropbox/C5/Scripts/GitKraken/CBE_selection/Functions.R')
source('C:/Users/Marta/Dropbox/C5/Scripts/GitKraken/CBE_selection/sim_functions.R')
source('C:/Users/Marta/Dropbox/C5/Scripts/GitKraken/CBE_selection/sim_functions_ce.R')
##################################################################################
# Differences with respect to previous versions
##################################################################################
# - consider less scenarios for OR2, p1, p2
# - add scenarios correlation between components
# - computation sample size according to E1
#########################################
# Preamble
#########################################
library(tidyverse)
library(tidyr)
library(plyr)
library(devtools)
install_github("CompARE-Composite/CompARE-package")
library(CompARE)
#########################################
# Define the set of scenarios
#########################################
# Scenarios
p0_e1 = c(0.1, 0.2)
p0_e2 = c(0.1, 0.25)
OR1 = c(0.6, 0.7, 0.8)
OR2 = c(0.75, 0.8)
scenarios = expand_grid(p0_e1,p0_e2,OR1,OR2)
scenarios$scenario = 1:dim(scenarios)[1]
# Probabilities treat group
scenarios$p1_e1 = (scenarios$OR1*scenarios$p0_e1/(1-scenarios$p0_e1))/(1+(scenarios$OR1*scenarios$p0_e1/(1-scenarios$p0_e1)))
scenarios$p1_e2 = (scenarios$OR2*scenarios$p0_e2/(1-scenarios$p0_e2))/(1+(scenarios$OR2*scenarios$p0_e2/(1-scenarios$p0_e2)))
# Calculate the correlation bounds
scenarios$min_corr0 = mapply(lower_corr,scenarios$p0_e1,scenarios$p0_e2)
scenarios$min_corr1 = mapply(lower_corr,scenarios$p1_e1,scenarios$p1_e2)
scenarios$max_corr0 = mapply(upper_corr,scenarios$p0_e1,scenarios$p0_e2)
scenarios$max_corr1 = mapply(upper_corr,scenarios$p1_e1,scenarios$p1_e2)
scenarios$min_corr = pmax(scenarios$min_corr0,scenarios$min_corr1)
scenarios$max_corr = pmin(scenarios$max_corr0,scenarios$max_corr1)
corr = c(0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8)
dataset = expand_grid(scenarios, corr)
dataset = subset(dataset, dataset$corr < dataset$max_corr & dataset$corr > dataset$min_corr)
# Calculate CE Probabilities
dataset$p0_ce = mapply(prob_cbe, p_e1=dataset$p0_e1, p_e2=dataset$p0_e2, rho=dataset$corr)
dataset$p1_ce = mapply(prob_cbe, p_e1=dataset$p1_e1, p_e2=dataset$p1_e2, rho=dataset$corr)
dataset$OR_ce = (dataset$p1_ce/(1-dataset$p1_ce))/(dataset$p0_ce/(1-dataset$p0_ce))
# Calculate ARE
dataset$ARE = mapply(ARE_cbe, p0_e1=dataset$p0_e1, p0_e2=dataset$p0_e2, eff_e1=dataset$OR1, eff_e2=dataset$OR2, rho=dataset$corr)
dataset$decision = ifelse(dataset$ARE<1, "RE", "CE")
# Calculate Sample size (total sample size, n=2*n0=2*n1)
alpha=0.05; beta=0.2
dataset$samplesize_e1 = mapply(samplesize_OR,p0=dataset$p0_e1, OR=dataset$OR1, alpha=alpha, beta=beta)
# dataset$samplesize_e1 = samplesize_OR(p0=dataset$p0_e1, OR=dataset$OR1, alpha=alpha, beta=beta)
rm(OR1,OR2,p0_e1,p0_e2,corr,scenarios)
# Vector empirical powers and significance level
dataset$Test_Power_CE <- NA
dataset$Test_Power_RE <- NA
dataset$Test_Power_ES <- NA
dataset$decision_ES <- NA
dataset$Test_Reject_CE <- NA
dataset$Test_Reject_RE <- NA
dataset$Test_Reject_ES <- NA
save.image("C:/Users/Marta/Dropbox/C5/Scripts/GitKraken/CBE_selection/scenarios.RData")
